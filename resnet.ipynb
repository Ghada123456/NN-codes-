{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hello, Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OHZ0s-JbkZpW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "_BATCH_NORM_DECAY = 0.997\n",
        "_BATCH_NORM_EPSILON = 1e-5\n",
        "DEFAULT_VERSION = 2\n",
        "DEFAULT_DTYPE = tf.float32\n",
        "CASTABLE_TYPES = (tf.float16,)\n",
        "ALLOWED_TYPES = (DEFAULT_DTYPE,) + CASTABLE_TYPES\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpjD_Gpekhly",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_norm(inputs, training, data_format):\n",
        " \n",
        "  return tf.compat.v1.layers.batch_normalization(\n",
        "      inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
        "      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n",
        "      scale=True, training=training, fused=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bj0HXGKklU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fixed_padding(inputs, kernel_size, data_format):\n",
        "  \n",
        "  pad_total = kernel_size - 1\n",
        "  pad_beg = pad_total // 2\n",
        "  pad_end = pad_total - pad_beg\n",
        "\n",
        "  if data_format == 'channels_first':\n",
        "    padded_inputs = tf.pad(tensor=inputs,\n",
        "                           paddings=[[0, 0], [0, 0], [pad_beg, pad_end],\n",
        "                                     [pad_beg, pad_end]])\n",
        "  else:\n",
        "    padded_inputs = tf.pad(tensor=inputs,\n",
        "                           paddings=[[0, 0], [pad_beg, pad_end],\n",
        "                                     [pad_beg, pad_end], [0, 0]])\n",
        "  return padded_inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tfNppEp_kuM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n",
        "\n",
        "  if strides > 1:\n",
        "    inputs = fixed_padding(inputs, kernel_size, data_format)\n",
        "\n",
        "  return tf.compat.v1.layers.conv2d(\n",
        "      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "      padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n",
        "      kernel_initializer=tf.compat.v1.variance_scaling_initializer(),\n",
        "      data_format=data_format)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "So0A_JKRkyB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _building_block_v1(inputs, filters, training, projection_shortcut, strides,\n",
        "                       data_format):\n",
        "\n",
        "  shortcut = inputs\n",
        "\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "    shortcut = batch_norm(inputs=shortcut, training=training,\n",
        "                          data_format=data_format)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs += shortcut\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  return inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QzspWomek8rJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _building_block_v2(inputs, filters, training, projection_shortcut, strides,\n",
        "                       data_format):\n",
        "\n",
        "  shortcut = inputs\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  # The projection shortcut should come after the first batch norm and ReLU\n",
        "  # since it performs a 1x1 convolution.\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  return inputs + shortcut\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7tr7WJZlDkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n",
        "                         strides, data_format):\n",
        "\n",
        "  shortcut = inputs\n",
        "\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "    shortcut = batch_norm(inputs=shortcut, training=training,\n",
        "                          data_format=data_format)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs += shortcut\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oif2SXQllJ0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _bottleneck_block_v2(inputs, filters, training, projection_shortcut,\n",
        "                         strides, data_format):\n",
        "\n",
        "  shortcut = inputs\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  # The projection shortcut should come after the first batch norm and ReLU\n",
        "  # since it performs a 1x1 convolution.\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  return inputs + shortcut\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRGkG8q0lPKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n",
        "                training, name, data_format):\n",
        "\n",
        "\n",
        "  # Bottleneck blocks end with 4x the number of filters as they start with\n",
        "  filters_out = filters * 4 if bottleneck else filters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbzsFEpNlUlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "a68b480f-8c94-4685-ae1e-ebe7218a4bce"
      },
      "cell_type": "code",
      "source": [
        "  def projection_shortcut(inputs):\n",
        "    return conv2d_fixed_padding(\n",
        "        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n",
        "        data_format=data_format)\n",
        "\n",
        "  # Only the first block per block_layer uses projection_shortcut and strides\n",
        "  inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n",
        "                    data_format)\n",
        "\n",
        "  for _ in range(1, blocks):\n",
        "    inputs = block_fn(inputs, filters, training, None, 1, data_format)\n",
        "\n",
        "  return tf.identity(inputs, name)\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ee58e38d255f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Only the first block per block_layer uses projection_shortcut and strides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n\u001b[0m\u001b[1;32m      8\u001b[0m                   data_format)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'block_fn' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wP7FnBU0lZbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "  \"\"\"Base class for building the Resnet Model.\"\"\"\n",
        "\n",
        "  def __init__(self, resnet_size, bottleneck, num_classes, num_filters,\n",
        "               kernel_size,\n",
        "               conv_stride, first_pool_size, first_pool_stride,\n",
        "               block_sizes, block_strides,\n",
        "               resnet_version=DEFAULT_VERSION, data_format=None,\n",
        "               dtype=DEFAULT_DTYPE):\n",
        "\n",
        "    self.resnet_size = resnet_size\n",
        "\n",
        "    if not data_format:\n",
        "      data_format = (\n",
        "          'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
        "\n",
        "    self.resnet_version = resnet_version\n",
        "    if resnet_version not in (1, 2):\n",
        "      raise ValueError(\n",
        "          'Resnet version should be 1 or 2. See README for citations.')\n",
        "\n",
        "    self.bottleneck = bottleneck\n",
        "    if bottleneck:\n",
        "      if resnet_version == 1:\n",
        "        self.block_fn = _bottleneck_block_v1\n",
        "      else:\n",
        "        self.block_fn = _bottleneck_block_v2\n",
        "    else:\n",
        "      if resnet_version == 1:\n",
        "        self.block_fn = _building_block_v1\n",
        "      else:\n",
        "        self.block_fn = _building_block_v2\n",
        "\n",
        "    if dtype not in ALLOWED_TYPES:\n",
        "      raise ValueError('dtype must be one of: {}'.format(ALLOWED_TYPES))\n",
        "\n",
        "    self.data_format = data_format\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.conv_stride = conv_stride\n",
        "    self.first_pool_size = first_pool_size\n",
        "    self.first_pool_stride = first_pool_stride\n",
        "    self.block_sizes = block_sizes\n",
        "    self.block_strides = block_strides\n",
        "    self.dtype = dtype\n",
        "    self.pre_activation = resnet_version == 2\n",
        "\n",
        "  def _custom_dtype_getter(self, getter, name, shape=None, dtype=DEFAULT_DTYPE,\n",
        "                           *args, **kwargs):\n",
        "    \n",
        "   \n",
        "\n",
        "    if dtype in CASTABLE_TYPES:\n",
        "      var = getter(name, shape, tf.float32, *args, **kwargs)\n",
        "      return tf.cast(var, dtype=dtype, name=name + '_cast')\n",
        "    else:\n",
        "      return getter(name, shape, dtype, *args, **kwargs)\n",
        "\n",
        "  def _model_variable_scope(self):\n",
        "    \"\"\"Returns a variable scope that the model should be created under.\n",
        "    If self.dtype is a castable type, model variable will be created in fp32\n",
        "    then cast to self.dtype before being used.\n",
        "    Returns:\n",
        "      A variable scope for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    return tf.compat.v1.variable_scope('resnet_model',\n",
        "                                       custom_getter=self._custom_dtype_getter)\n",
        "\n",
        "  def __call__(self, inputs, training):\n",
        "   \n",
        "\n",
        "    with self._model_variable_scope():\n",
        "      if self.data_format == 'channels_first':\n",
        "       \n",
        "        inputs = tf.transpose(a=inputs, perm=[0, 3, 1, 2])\n",
        "\n",
        "      inputs = conv2d_fixed_padding(\n",
        "          inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,\n",
        "          strides=self.conv_stride, data_format=self.data_format)\n",
        "      inputs = tf.identity(inputs, 'initial_conv')\n",
        "\n",
        "    \n",
        "      if self.resnet_version == 1:\n",
        "        inputs = batch_norm(inputs, training, self.data_format)\n",
        "        inputs = tf.nn.relu(inputs)\n",
        "\n",
        "      if self.first_pool_size:\n",
        "        inputs = tf.compat.v1.layers.max_pooling2d(\n",
        "            inputs=inputs, pool_size=self.first_pool_size,\n",
        "            strides=self.first_pool_stride, padding='SAME',\n",
        "            data_format=self.data_format)\n",
        "        inputs = tf.identity(inputs, 'initial_max_pool')\n",
        "\n",
        "      for i, num_blocks in enumerate(self.block_sizes):\n",
        "        num_filters = self.num_filters * (2**i)\n",
        "        inputs = block_layer(\n",
        "            inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n",
        "            block_fn=self.block_fn, blocks=num_blocks,\n",
        "            strides=self.block_strides[i], training=training,\n",
        "            name='block_layer{}'.format(i + 1), data_format=self.data_format)\n",
        "\n",
        "     \n",
        "      if self.pre_activation:\n",
        "        inputs = batch_norm(inputs, training, self.data_format)\n",
        "        inputs = tf.nn.relu(inputs)\n",
        "\n",
        "     \n",
        "      axes = [2, 3] if self.data_format == 'channels_first' else [1, 2]\n",
        "      inputs = tf.reduce_mean(input_tensor=inputs, axis=axes, keepdims=True)\n",
        "      inputs = tf.identity(inputs, 'final_reduce_mean')\n",
        "\n",
        "      inputs = tf.squeeze(inputs, axes)\n",
        "      inputs = tf.compat.v1.layers.dense(inputs=inputs, units=self.num_classes)\n",
        "      inputs = tf.identity(inputs, 'final_dense')\n",
        "      return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bsO35wyFl30v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "0338e078-fa7e-477d-f3ab-1c45c40fd36f"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "# pylint: disable=g-bad-import-order\n",
        "from absl import flags\n",
        "import tensorflow as tf\n",
        "\n",
        "from official.resnet import resnet_model\n",
        "from official.utils.flags import core as flags_core\n",
        "from official.utils.export import export\n",
        "from official.utils.logs import hooks_helper\n",
        "from official.utils.logs import logger\n",
        "from official.resnet import imagenet_preprocessing\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.utils.misc import model_helpers\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Functions for input processing.\n",
        "################################################################################\n",
        "def process_record_dataset(dataset,\n",
        "                           is_training,\n",
        "                           batch_size,\n",
        "                           shuffle_buffer,\n",
        "                           parse_record_fn,\n",
        "                           num_epochs=1,\n",
        "                           dtype=tf.float32,\n",
        "                           datasets_num_private_threads=None,\n",
        "                           num_parallel_batches=1):\n",
        "  \"\"\"Given a Dataset with raw records, return an iterator over the records.\n",
        "  Args:\n",
        "    dataset: A Dataset representing raw records\n",
        "    is_training: A boolean denoting whether the input is for training.\n",
        "    batch_size: The number of samples per batch.\n",
        "    shuffle_buffer: The buffer size to use when shuffling records. A larger\n",
        "      value results in better randomness, but smaller values reduce startup\n",
        "      time and use less memory.\n",
        "    parse_record_fn: A function that takes a raw record and returns the\n",
        "      corresponding (image, label) pair.\n",
        "    num_epochs: The number of epochs to repeat the dataset.\n",
        "    dtype: Data type to use for images/features.\n",
        "    datasets_num_private_threads: Number of threads for a private\n",
        "      threadpool created for all datasets computation.\n",
        "    num_parallel_batches: Number of parallel batches for tf.data.\n",
        "  Returns:\n",
        "    Dataset of (image, label) pairs ready for iteration.\n",
        "  \"\"\"\n",
        "  # Defines a specific size thread pool for tf.data operations.\n",
        "  if datasets_num_private_threads:\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_threading.private_threadpool_size = (\n",
        "        datasets_num_private_threads)\n",
        "    dataset = dataset.with_options(options)\n",
        "    tf.compat.v1.logging.info('datasets_num_private_threads: %s',\n",
        "                              datasets_num_private_threads)\n",
        "\n",
        "  # Prefetches a batch at a time to smooth out the time taken to load input\n",
        "  # files for shuffling and processing.\n",
        "  dataset = dataset.prefetch(buffer_size=batch_size)\n",
        "  if is_training:\n",
        "    # Shuffles records before repeating to respect epoch boundaries.\n",
        "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
        "\n",
        "  # Repeats the dataset for the number of epochs to train.\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "\n",
        "  # Parses the raw records into images and labels.\n",
        "  dataset = dataset.apply(\n",
        "      tf.data.experimental.map_and_batch(\n",
        "          lambda value: parse_record_fn(value, is_training, dtype),\n",
        "          batch_size=batch_size,\n",
        "          num_parallel_batches=num_parallel_batches,\n",
        "          drop_remainder=False))\n",
        "\n",
        "  # Operations between the final prefetch and the get_next call to the iterator\n",
        "  # will happen synchronously during run time. We prefetch here again to\n",
        "  # background all of the above processing work and keep it out of the\n",
        "  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n",
        "  # allows DistributionStrategies to adjust how many batches to fetch based\n",
        "  # on how many devices are present.\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_synth_input_fn(height, width, num_channels, num_classes,\n",
        "                       dtype=tf.float32):\n",
        "  \"\"\"Returns an input function that returns a dataset with random data.\n",
        "  This input_fn returns a data set that iterates over a set of random data and\n",
        "  bypasses all preprocessing, e.g. jpeg decode and copy. The host to device\n",
        "  copy is still included. This used to find the upper throughput bound when\n",
        "  tunning the full input pipeline.\n",
        "  Args:\n",
        "    height: Integer height that will be used to create a fake image tensor.\n",
        "    width: Integer width that will be used to create a fake image tensor.\n",
        "    num_channels: Integer depth that will be used to create a fake image tensor.\n",
        "    num_classes: Number of classes that should be represented in the fake labels\n",
        "      tensor\n",
        "    dtype: Data type for features/images.\n",
        "  Returns:\n",
        "    An input_fn that can be used in place of a real one to return a dataset\n",
        "    that can be used for iteration.\n",
        "  \"\"\"\n",
        "  # pylint: disable=unused-argument\n",
        "  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):\n",
        "    \"\"\"Returns dataset filled with random data.\"\"\"\n",
        "    # Synthetic input should be within [0, 255].\n",
        "    inputs = tf.random.truncated_normal(\n",
        "        [batch_size] + [height, width, num_channels],\n",
        "        dtype=dtype,\n",
        "        mean=127,\n",
        "        stddev=60,\n",
        "        name='synthetic_inputs')\n",
        "\n",
        "    labels = tf.random.uniform(\n",
        "        [batch_size],\n",
        "        minval=0,\n",
        "        maxval=num_classes - 1,\n",
        "        dtype=tf.int32,\n",
        "        name='synthetic_labels')\n",
        "    data = tf.data.Dataset.from_tensors((inputs, labels)).repeat()\n",
        "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def image_bytes_serving_input_fn(image_shape, dtype=tf.float32):\n",
        "  \"\"\"Serving input fn for raw jpeg images.\"\"\"\n",
        "\n",
        "  def _preprocess_image(image_bytes):\n",
        "    \"\"\"Preprocess a single raw image.\"\"\"\n",
        "    # Bounding box around the whole image.\n",
        "    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=dtype, shape=[1, 1, 4])\n",
        "    height, width, num_channels = image_shape\n",
        "    image = imagenet_preprocessing.preprocess_image(\n",
        "        image_bytes, bbox, height, width, num_channels, is_training=False)\n",
        "    return image\n",
        "\n",
        "  image_bytes_list = tf.compat.v1.placeholder(\n",
        "      shape=[None], dtype=tf.string, name='input_tensor')\n",
        "  images = tf.map_fn(\n",
        "      _preprocess_image, image_bytes_list, back_prop=False, dtype=dtype)\n",
        "  return tf.estimator.export.TensorServingInputReceiver(\n",
        "      images, {'image_bytes': image_bytes_list})\n",
        "\n",
        "\n",
        "def override_flags_and_set_envars_for_gpu_thread_pool(flags_obj):\n",
        "  \"\"\"Override flags and set env_vars for performance.\n",
        "  These settings exist to test the difference between using stock settings\n",
        "  and manual tuning. It also shows some of the ENV_VARS that can be tweaked to\n",
        "  squeeze a few extra examples per second.  These settings are defaulted to the\n",
        "  current platform of interest, which changes over time.\n",
        "  On systems with small numbers of cpu cores, e.g. under 8 logical cores,\n",
        "  setting up a gpu thread pool with `tf_gpu_thread_mode=gpu_private` may perform\n",
        "  poorly.\n",
        "  Args:\n",
        "    flags_obj: Current flags, which will be adjusted possibly overriding\n",
        "    what has been set by the user on the command-line.\n",
        "  \"\"\"\n",
        "  cpu_count = multiprocessing.cpu_count()\n",
        "  tf.compat.v1.logging.info('Logical CPU cores: %s', cpu_count)\n",
        "\n",
        "  # Sets up thread pool for each GPU for op scheduling.\n",
        "  per_gpu_thread_count = 1\n",
        "  total_gpu_thread_count = per_gpu_thread_count * flags_obj.num_gpus\n",
        "  os.environ['TF_GPU_THREAD_MODE'] = flags_obj.tf_gpu_thread_mode\n",
        "  os.environ['TF_GPU_THREAD_COUNT'] = str(per_gpu_thread_count)\n",
        "  tf.compat.v1.logging.info('TF_GPU_THREAD_COUNT: %s',\n",
        "                            os.environ['TF_GPU_THREAD_COUNT'])\n",
        "  tf.compat.v1.logging.info('TF_GPU_THREAD_MODE: %s',\n",
        "                            os.environ['TF_GPU_THREAD_MODE'])\n",
        "\n",
        "  # Reduces general thread pool by number of threads used for GPU pool.\n",
        "  main_thread_count = cpu_count - total_gpu_thread_count\n",
        "  flags_obj.inter_op_parallelism_threads = main_thread_count\n",
        "\n",
        "  # Sets thread count for tf.data. Logical cores minus threads assign to the\n",
        "  # private GPU pool along with 2 thread per GPU for event monitoring and\n",
        "  # sending / receiving tensors.\n",
        "  num_monitoring_threads = 2 * flags_obj.num_gpus\n",
        "  flags_obj.datasets_num_private_threads = (cpu_count - total_gpu_thread_count\n",
        "                                            - num_monitoring_threads)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Functions for running training/eval/validation loops for the model.\n",
        "################################################################################\n",
        "def learning_rate_with_decay(\n",
        "    batch_size, batch_denom, num_images, boundary_epochs, decay_rates,\n",
        "    base_lr=0.1, warmup=False):\n",
        "  \"\"\"Get a learning rate that decays step-wise as training progresses.\n",
        "  Args:\n",
        "    batch_size: the number of examples processed in each training batch.\n",
        "    batch_denom: this value will be used to scale the base learning rate.\n",
        "      `0.1 * batch size` is divided by this number, such that when\n",
        "      batch_denom == batch_size, the initial learning rate will be 0.1.\n",
        "    num_images: total number of images that will be used for training.\n",
        "    boundary_epochs: list of ints representing the epochs at which we\n",
        "      decay the learning rate.\n",
        "    decay_rates: list of floats representing the decay rates to be used\n",
        "      for scaling the learning rate. It should have one more element\n",
        "      than `boundary_epochs`, and all elements should have the same type.\n",
        "    base_lr: Initial learning rate scaled based on batch_denom.\n",
        "    warmup: Run a 5 epoch warmup to the initial lr.\n",
        "  Returns:\n",
        "    Returns a function that takes a single argument - the number of batches\n",
        "    trained so far (global_step)- and returns the learning rate to be used\n",
        "    for training the next batch.\n",
        "  \"\"\"\n",
        "  initial_learning_rate = base_lr * batch_size / batch_denom\n",
        "  batches_per_epoch = num_images / batch_size\n",
        "\n",
        "  # Reduce the learning rate at certain epochs.\n",
        "  # CIFAR-10: divide by 10 at epoch 100, 150, and 200\n",
        "  # ImageNet: divide by 10 at epoch 30, 60, 80, and 90\n",
        "  boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
        "  vals = [initial_learning_rate * decay for decay in decay_rates]\n",
        "\n",
        "  def learning_rate_fn(global_step):\n",
        "    \"\"\"Builds scaled learning rate function with 5 epoch warm up.\"\"\"\n",
        "    lr = tf.compat.v1.train.piecewise_constant(global_step, boundaries, vals)\n",
        "    if warmup:\n",
        "      warmup_steps = int(batches_per_epoch * 5)\n",
        "      warmup_lr = (\n",
        "          initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(\n",
        "              warmup_steps, tf.float32))\n",
        "      return tf.cond(pred=global_step < warmup_steps,\n",
        "                     true_fn=lambda: warmup_lr,\n",
        "                     false_fn=lambda: lr)\n",
        "    return lr\n",
        "\n",
        "  return learning_rate_fn\n",
        "\n",
        "\n",
        "def resnet_model_fn(features, labels, mode, model_class,\n",
        "                    resnet_size, weight_decay, learning_rate_fn, momentum,\n",
        "                    data_format, resnet_version, loss_scale,\n",
        "                    loss_filter_fn=None, dtype=resnet_model.DEFAULT_DTYPE,\n",
        "                    fine_tune=False):\n",
        "  \"\"\"Shared functionality for different resnet model_fns.\n",
        "  Initializes the ResnetModel representing the model layers\n",
        "  and uses that model to build the necessary EstimatorSpecs for\n",
        "  the `mode` in question. For training, this means building losses,\n",
        "  the optimizer, and the train op that get passed into the EstimatorSpec.\n",
        "  For evaluation and prediction, the EstimatorSpec is returned without\n",
        "  a train op, but with the necessary parameters for the given mode.\n",
        "  Args:\n",
        "    features: tensor representing input images\n",
        "    labels: tensor representing class labels for all input images\n",
        "    mode: current estimator mode; should be one of\n",
        "      `tf.estimator.ModeKeys.TRAIN`, `EVALUATE`, `PREDICT`\n",
        "    model_class: a class representing a TensorFlow model that has a __call__\n",
        "      function. We assume here that this is a subclass of ResnetModel.\n",
        "    resnet_size: A single integer for the size of the ResNet model.\n",
        "    weight_decay: weight decay loss rate used to regularize learned variables.\n",
        "    learning_rate_fn: function that returns the current learning rate given\n",
        "      the current global_step\n",
        "    momentum: momentum term used for optimization\n",
        "    data_format: Input format ('channels_last', 'channels_first', or None).\n",
        "      If set to None, the format is dependent on whether a GPU is available.\n",
        "    resnet_version: Integer representing which version of the ResNet network to\n",
        "      use. See README for details. Valid values: [1, 2]\n",
        "    loss_scale: The factor to scale the loss for numerical stability. A detailed\n",
        "      summary is present in the arg parser help text.\n",
        "    loss_filter_fn: function that takes a string variable name and returns\n",
        "      True if the var should be included in loss calculation, and False\n",
        "      otherwise. If None, batch_normalization variables will be excluded\n",
        "      from the loss.\n",
        "    dtype: the TensorFlow dtype to use for calculations.\n",
        "    fine_tune: If True only train the dense layers(final layers).\n",
        "  Returns:\n",
        "    EstimatorSpec parameterized according to the input params and the\n",
        "    current mode.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate a summary node for the images\n",
        "  tf.compat.v1.summary.image('images', features, max_outputs=6)\n",
        "  # Checks that features/images have same data type being used for calculations.\n",
        "  assert features.dtype == dtype\n",
        "\n",
        "  model = model_class(resnet_size, data_format, resnet_version=resnet_version,\n",
        "                      dtype=dtype)\n",
        "\n",
        "  logits = model(features, mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  # This acts as a no-op if the logits are already in fp32 (provided logits are\n",
        "  # not a SparseTensor). If dtype is is low precision, logits must be cast to\n",
        "  # fp32 for numerical stability.\n",
        "  logits = tf.cast(logits, tf.float32)\n",
        "\n",
        "  predictions = {\n",
        "      'classes': tf.argmax(input=logits, axis=1),\n",
        "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    # Return the predictions and the specification for serving a SavedModel\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=predictions,\n",
        "        export_outputs={\n",
        "            'predict': tf.estimator.export.PredictOutput(predictions)\n",
        "        })\n",
        "\n",
        "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
        "  cross_entropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
        "      logits=logits, labels=labels)\n",
        "\n",
        "  # Create a tensor named cross_entropy for logging purposes.\n",
        "  tf.identity(cross_entropy, name='cross_entropy')\n",
        "  tf.compat.v1.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "  # If no loss_filter_fn is passed, assume we want the default behavior,\n",
        "  # which is that batch_normalization variables are excluded from loss.\n",
        "  def exclude_batch_norm(name):\n",
        "    return 'batch_normalization' not in name\n",
        "  loss_filter_fn = loss_filter_fn or exclude_batch_norm\n",
        "\n",
        "  # Add weight decay to the loss. We need to scale the regularization loss\n",
        "  # manually as losses other than in tf.losses and tf.keras.losses don't scale\n",
        "  # automatically.\n",
        "  l2_loss = weight_decay * tf.add_n(\n",
        "      # loss is computed using fp32 for numerical stability.\n",
        "      [\n",
        "          tf.nn.l2_loss(tf.cast(v, tf.float32))\n",
        "          for v in tf.compat.v1.trainable_variables()\n",
        "          if loss_filter_fn(v.name)\n",
        "      ]) / tf.distribute.get_strategy().num_replicas_in_sync\n",
        "  tf.compat.v1.summary.scalar('l2_loss', l2_loss)\n",
        "  loss = cross_entropy + l2_loss\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "    learning_rate = learning_rate_fn(global_step)\n",
        "\n",
        "    # Create a tensor named learning_rate for logging purposes\n",
        "    tf.identity(learning_rate, name='learning_rate')\n",
        "    tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "    optimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        momentum=momentum\n",
        "    )\n",
        "\n",
        "    def _dense_grad_filter(gvs):\n",
        "      \"\"\"Only apply gradient updates to the final layer.\n",
        "      This function is used for fine tuning.\n",
        "      Args:\n",
        "        gvs: list of tuples with gradients and variable info\n",
        "      Returns:\n",
        "        filtered gradients so that only the dense layer remains\n",
        "      \"\"\"\n",
        "      return [(g, v) for g, v in gvs if 'dense' in v.name]\n",
        "\n",
        "    if loss_scale != 1:\n",
        "      # When computing fp16 gradients, often intermediate tensor values are\n",
        "      # so small, they underflow to 0. To avoid this, we multiply the loss by\n",
        "      # loss_scale to make these tensor values loss_scale times bigger.\n",
        "      scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n",
        "\n",
        "      if fine_tune:\n",
        "        scaled_grad_vars = _dense_grad_filter(scaled_grad_vars)\n",
        "\n",
        "      # Once the gradient computation is complete we can scale the gradients\n",
        "      # back to the correct scale before passing them to the optimizer.\n",
        "      unscaled_grad_vars = [(grad / loss_scale, var)\n",
        "                            for grad, var in scaled_grad_vars]\n",
        "      minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)\n",
        "    else:\n",
        "      grad_vars = optimizer.compute_gradients(loss)\n",
        "      if fine_tune:\n",
        "        grad_vars = _dense_grad_filter(grad_vars)\n",
        "      minimize_op = optimizer.apply_gradients(grad_vars, global_step)\n",
        "\n",
        "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "    train_op = tf.group(minimize_op, update_ops)\n",
        "  else:\n",
        "    train_op = None\n",
        "\n",
        "  accuracy = tf.compat.v1.metrics.accuracy(labels, predictions['classes'])\n",
        "  accuracy_top_5 = tf.compat.v1.metrics.mean(\n",
        "      tf.nn.in_top_k(predictions=logits, targets=labels, k=5, name='top_5_op'))\n",
        "  metrics = {'accuracy': accuracy,\n",
        "             'accuracy_top_5': accuracy_top_5}\n",
        "\n",
        "  # Create a tensor named train_accuracy for logging purposes\n",
        "  tf.identity(accuracy[1], name='train_accuracy')\n",
        "  tf.identity(accuracy_top_5[1], name='train_accuracy_top_5')\n",
        "  tf.compat.v1.summary.scalar('train_accuracy', accuracy[1])\n",
        "  tf.compat.v1.summary.scalar('train_accuracy_top_5', accuracy_top_5[1])\n",
        "\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      predictions=predictions,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops=metrics)\n",
        "\n",
        "\n",
        "def resnet_main(\n",
        "    flags_obj, model_function, input_function, dataset_name, shape=None):\n",
        "  \"\"\"Shared main loop for ResNet Models.\n",
        "  Args:\n",
        "    flags_obj: An object containing parsed flags. See define_resnet_flags()\n",
        "      for details.\n",
        "    model_function: the function that instantiates the Model and builds the\n",
        "      ops for train/eval. This will be passed directly into the estimator.\n",
        "    input_function: the function that processes the dataset and returns a\n",
        "      dataset that the estimator can train on. This will be wrapped with\n",
        "      all the relevant flags for running and passed to estimator.\n",
        "    dataset_name: the name of the dataset for training and evaluation. This is\n",
        "      used for logging purpose.\n",
        "    shape: list of ints representing the shape of the images used for training.\n",
        "      This is only used if flags_obj.export_dir is passed.\n",
        "  Returns:\n",
        "    Dict of results of the run.\n",
        "  \"\"\"\n",
        "\n",
        "  model_helpers.apply_clean(flags.FLAGS)\n",
        "\n",
        "  # Ensures flag override logic is only executed if explicitly triggered.\n",
        "  if flags_obj.tf_gpu_thread_mode:\n",
        "    override_flags_and_set_envars_for_gpu_thread_pool(flags_obj)\n",
        "\n",
        "  # Configures cluster spec for distribution strategy.\n",
        "  num_workers = distribution_utils.configure_cluster(flags_obj.worker_hosts,\n",
        "                                                     flags_obj.task_index)\n",
        "\n",
        "  # Creates session config. allow_soft_placement = True, is required for\n",
        "  # multi-GPU and is not harmful for other modes.\n",
        "  session_config = tf.compat.v1.ConfigProto(\n",
        "      inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,\n",
        "      intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,\n",
        "      allow_soft_placement=True)\n",
        "\n",
        "  distribution_strategy = distribution_utils.get_distribution_strategy(\n",
        "      distribution_strategy=flags_obj.distribution_strategy,\n",
        "      num_gpus=flags_core.get_num_gpus(flags_obj),\n",
        "      num_workers=num_workers,\n",
        "      all_reduce_alg=flags_obj.all_reduce_alg)\n",
        "\n",
        "  # Creates a `RunConfig` that checkpoints every 24 hours which essentially\n",
        "  # results in checkpoints determined only by `epochs_between_evals`.\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      train_distribute=distribution_strategy,\n",
        "      session_config=session_config,\n",
        "      save_checkpoints_secs=60*60*24)\n",
        "\n",
        "  # Initializes model with all but the dense layer from pretrained ResNet.\n",
        "  if flags_obj.pretrained_model_checkpoint_path is not None:\n",
        "    warm_start_settings = tf.estimator.WarmStartSettings(\n",
        "        flags_obj.pretrained_model_checkpoint_path,\n",
        "        vars_to_warm_start='^(?!.*dense)')\n",
        "  else:\n",
        "    warm_start_settings = None\n",
        "\n",
        "  classifier = tf.estimator.Estimator(\n",
        "      model_fn=model_function, model_dir=flags_obj.model_dir, config=run_config,\n",
        "      warm_start_from=warm_start_settings, params={\n",
        "          'resnet_size': int(flags_obj.resnet_size),\n",
        "          'data_format': flags_obj.data_format,\n",
        "          'batch_size': flags_obj.batch_size,\n",
        "          'resnet_version': int(flags_obj.resnet_version),\n",
        "          'loss_scale': flags_core.get_loss_scale(flags_obj),\n",
        "          'dtype': flags_core.get_tf_dtype(flags_obj),\n",
        "          'fine_tune': flags_obj.fine_tune\n",
        "      })\n",
        "\n",
        "  run_params = {\n",
        "      'batch_size': flags_obj.batch_size,\n",
        "      'dtype': flags_core.get_tf_dtype(flags_obj),\n",
        "      'resnet_size': flags_obj.resnet_size,\n",
        "      'resnet_version': flags_obj.resnet_version,\n",
        "      'synthetic_data': flags_obj.use_synthetic_data,\n",
        "      'train_epochs': flags_obj.train_epochs,\n",
        "  }\n",
        "  if flags_obj.use_synthetic_data:\n",
        "    dataset_name = dataset_name + '-synthetic'\n",
        "\n",
        "  benchmark_logger = logger.get_benchmark_logger()\n",
        "  benchmark_logger.log_run_info('resnet', dataset_name, run_params,\n",
        "                                test_id=flags_obj.benchmark_test_id)\n",
        "\n",
        "  train_hooks = hooks_helper.get_train_hooks(\n",
        "      flags_obj.hooks,\n",
        "      model_dir=flags_obj.model_dir,\n",
        "      batch_size=flags_obj.batch_size)\n",
        "\n",
        "  def input_fn_train(num_epochs):\n",
        "    return input_function(\n",
        "        is_training=True,\n",
        "        data_dir=flags_obj.data_dir,\n",
        "        batch_size=distribution_utils.per_device_batch_size(\n",
        "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
        "        num_epochs=num_epochs,\n",
        "        dtype=flags_core.get_tf_dtype(flags_obj),\n",
        "        datasets_num_private_threads=flags_obj.datasets_num_private_threads,\n",
        "        num_parallel_batches=flags_obj.datasets_num_parallel_batches)\n",
        "\n",
        "  def input_fn_eval():\n",
        "    return input_function(\n",
        "        is_training=False,\n",
        "        data_dir=flags_obj.data_dir,\n",
        "        batch_size=distribution_utils.per_device_batch_size(\n",
        "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
        "        num_epochs=1,\n",
        "        dtype=flags_core.get_tf_dtype(flags_obj))\n",
        "\n",
        "  train_epochs = (0 if flags_obj.eval_only or not flags_obj.train_epochs else\n",
        "                  flags_obj.train_epochs)\n",
        "\n",
        "  use_train_and_evaluate = flags_obj.use_train_and_evaluate or (\n",
        "      distribution_strategy.__class__.__name__ == 'CollectiveAllReduceStrategy')\n",
        "  if use_train_and_evaluate:\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn=lambda: input_fn_train(train_epochs), hooks=train_hooks,\n",
        "        max_steps=flags_obj.max_train_steps)\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn_eval,\n",
        "                                      steps=flags_obj.max_train_steps)\n",
        "    tf.compat.v1.logging.info('Starting to train and evaluate.')\n",
        "    eval_results, _ = tf.estimator.train_and_evaluate(classifier, train_spec,\n",
        "                                                      eval_spec)\n",
        "    benchmark_logger.log_evaluation_result(eval_results)\n",
        "  else:\n",
        "    if train_epochs == 0:\n",
        "      # If --eval_only is set, perform a single loop with zero train epochs.\n",
        "      schedule, n_loops = [0], 1\n",
        "    else:\n",
        "      # Compute the number of times to loop while training. All but the last\n",
        "      # pass will train for `epochs_between_evals` epochs, while the last will\n",
        "      # train for the number needed to reach `training_epochs`. For instance if\n",
        "      #   train_epochs = 25 and epochs_between_evals = 10\n",
        "      # schedule will be set to [10, 10, 5]. That is to say, the loop will:\n",
        "      #   Train for 10 epochs and then evaluate.\n",
        "      #   Train for another 10 epochs and then evaluate.\n",
        "      #   Train for a final 5 epochs (to reach 25 epochs) and then evaluate.\n",
        "      n_loops = math.ceil(train_epochs / flags_obj.epochs_between_evals)\n",
        "      schedule = [flags_obj.epochs_between_evals for _ in range(int(n_loops))]\n",
        "      schedule[-1] = train_epochs - sum(schedule[:-1])  # over counting.\n",
        "\n",
        "    for cycle_index, num_train_epochs in enumerate(schedule):\n",
        "      tf.compat.v1.logging.info('Starting cycle: %d/%d', cycle_index,\n",
        "                                int(n_loops))\n",
        "\n",
        "      if num_train_epochs:\n",
        "        # Since we are calling classifier.train immediately in each loop, the\n",
        "        # value of num_train_epochs in the lambda function will not be changed\n",
        "        # before it is used. So it is safe to ignore the pylint error here\n",
        "        # pylint: disable=cell-var-from-loop\n",
        "        classifier.train(input_fn=lambda: input_fn_train(num_train_epochs),\n",
        "                         hooks=train_hooks, max_steps=flags_obj.max_train_steps)\n",
        "\n",
        "      # flags_obj.max_train_steps is generally associated with testing and\n",
        "      # profiling. As a result it is frequently called with synthetic data,\n",
        "      # which will iterate forever. Passing steps=flags_obj.max_train_steps\n",
        "      # allows the eval (which is generally unimportant in those circumstances)\n",
        "      # to terminate.  Note that eval will run for max_train_steps each loop,\n",
        "      # regardless of the global_step count.\n",
        "      tf.compat.v1.logging.info('Starting to evaluate.')\n",
        "      eval_results = classifier.evaluate(input_fn=input_fn_eval,\n",
        "                                         steps=flags_obj.max_train_steps)\n",
        "\n",
        "      benchmark_logger.log_evaluation_result(eval_results)\n",
        "\n",
        "      if model_helpers.past_stop_threshold(\n",
        "          flags_obj.stop_threshold, eval_results['accuracy']):\n",
        "        break\n",
        "\n",
        "  if flags_obj.export_dir is not None:\n",
        "    # Exports a saved model for the given classifier.\n",
        "    export_dtype = flags_core.get_tf_dtype(flags_obj)\n",
        "    if flags_obj.image_bytes_as_serving_input:\n",
        "      input_receiver_fn = functools.partial(\n",
        "          image_bytes_serving_input_fn, shape, dtype=export_dtype)\n",
        "    else:\n",
        "      input_receiver_fn = export.build_tensor_serving_input_receiver_fn(\n",
        "          shape, batch_size=flags_obj.batch_size, dtype=export_dtype)\n",
        "    classifier.export_savedmodel(flags_obj.export_dir, input_receiver_fn,\n",
        "                                 strip_default_attrs=True)\n",
        "\n",
        "  stats = {}\n",
        "  stats['eval_results'] = eval_results\n",
        "  stats['train_hooks'] = train_hooks\n",
        "\n",
        "  return stats\n",
        "\n",
        "\n",
        "def define_resnet_flags(resnet_size_choices=None):\n",
        "  \"\"\"Add flags and validators for ResNet.\"\"\"\n",
        "  flags_core.define_base()\n",
        "  flags_core.define_performance(num_parallel_calls=False,\n",
        "                                tf_gpu_thread_mode=True,\n",
        "                                datasets_num_private_threads=True,\n",
        "                                datasets_num_parallel_batches=True)\n",
        "  flags_core.define_image()\n",
        "  flags_core.define_benchmark()\n",
        "  flags.adopt_module_key_flags(flags_core)\n",
        "\n",
        "  flags.DEFINE_enum(\n",
        "      name='resnet_version', short_name='rv', default='1',\n",
        "      enum_values=['1', '2'],\n",
        "      help=flags_core.help_wrap(\n",
        "          'Version of ResNet. (1 or 2) See README.md for details.'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='fine_tune', short_name='ft', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True do not train any parameters except for the final layer.'))\n",
        "  flags.DEFINE_string(\n",
        "      name='pretrained_model_checkpoint_path', short_name='pmcp', default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If not None initialize all the network except the final layer with '\n",
        "          'these values'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='eval_only', default=False,\n",
        "      help=flags_core.help_wrap('Skip training and only perform evaluation on '\n",
        "                                'the latest checkpoint.'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='image_bytes_as_serving_input', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True exports savedmodel with serving signature that accepts '\n",
        "          'JPEG image bytes instead of a fixed size [HxWxC] tensor that '\n",
        "          'represents the image. The former is easier to use for serving at '\n",
        "          'the expense of image resize/cropping being done as part of model '\n",
        "          'inference. Note, this flag only applies to ImageNet and cannot '\n",
        "          'be used for CIFAR.'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='use_train_and_evaluate', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True, uses `tf.estimator.train_and_evaluate` for the training '\n",
        "          'and evaluation loop, instead of separate calls to `classifier.train '\n",
        "          'and `classifier.evaluate`, which is the default behavior.'))\n",
        "  flags.DEFINE_string(\n",
        "      name='worker_hosts', default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Comma-separated list of worker ip:port pairs for running '\n",
        "          'multi-worker models with DistributionStrategy.  The user would '\n",
        "          'start the program on each host with identical value for this flag.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='task_index', default=-1,\n",
        "      help=flags_core.help_wrap('If multi-worker training, the task_index of '\n",
        "                                'this worker.'))\n",
        "  choice_kwargs = dict(\n",
        "      name='resnet_size', short_name='rs', default='50',\n",
        "      help=flags_core.help_wrap('The size of the ResNet model to use.'))\n",
        "\n",
        "  if resnet_size_choices is None:\n",
        "    flags.DEFINE_string(**choice_kwargs)\n",
        "  else:\n",
        "    flags.DEFINE_enum(enum_values=resnet_size_choices, **choice_kwargs)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-811c96f4dea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflags_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'official'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZgbK3swGmBVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "97965cdc-8279-4004-d5a5-374acbae5aab"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "from absl import app as absl_app\n",
        "from absl import flags\n",
        "import tensorflow as tf  # pylint: disable=g-bad-import-order\n",
        "\n",
        "from official.utils.flags import core as flags_core\n",
        "from official.utils.logs import logger\n",
        "from official.resnet import imagenet_preprocessing\n",
        "from official.resnet import resnet_model\n",
        "from official.resnet import resnet_run_loop\n",
        "\n",
        "DEFAULT_IMAGE_SIZE = 224\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 1001\n",
        "\n",
        "NUM_IMAGES = {\n",
        "    'train': 1281167,\n",
        "    'validation': 50000,\n",
        "}\n",
        "\n",
        "_NUM_TRAIN_FILES = 1024\n",
        "_SHUFFLE_BUFFER = 10000\n",
        "\n",
        "DATASET_NAME = 'ImageNet'\n",
        "\n",
        "###############################################################################\n",
        "# Data processing\n",
        "###############################################################################\n",
        "def get_filenames(is_training, data_dir):\n",
        "  \"\"\"Return filenames for dataset.\"\"\"\n",
        "  if is_training:\n",
        "    return [\n",
        "        os.path.join(data_dir, 'train-%05d-of-01024' % i)\n",
        "        for i in range(_NUM_TRAIN_FILES)]\n",
        "  else:\n",
        "    return [\n",
        "        os.path.join(data_dir, 'validation-%05d-of-00128' % i)\n",
        "        for i in range(128)]\n",
        "\n",
        "\n",
        "def _parse_example_proto(example_serialized):\n",
        "  \"\"\"Parses an Example proto containing a training example of an image.\n",
        "  The output of the build_image_data.py image preprocessing script is a dataset\n",
        "  containing serialized Example protocol buffers. Each Example proto contains\n",
        "  the following fields (values are included as examples):\n",
        "    image/height: 462\n",
        "    image/width: 581\n",
        "    image/colorspace: 'RGB'\n",
        "    image/channels: 3\n",
        "    image/class/label: 615\n",
        "    image/class/synset: 'n03623198'\n",
        "    image/class/text: 'knee pad'\n",
        "    image/object/bbox/xmin: 0.1\n",
        "    image/object/bbox/xmax: 0.9\n",
        "    image/object/bbox/ymin: 0.2\n",
        "    image/object/bbox/ymax: 0.6\n",
        "    image/object/bbox/label: 615\n",
        "    image/format: 'JPEG'\n",
        "    image/filename: 'ILSVRC2012_val_00041207.JPEG'\n",
        "    image/encoded: <JPEG encoded string>\n",
        "  Args:\n",
        "    example_serialized: scalar Tensor tf.string containing a serialized\n",
        "      Example protocol buffer.\n",
        "  Returns:\n",
        "    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n",
        "    label: Tensor tf.int32 containing the label.\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
        "      [ymin, xmin, ymax, xmax].\n",
        "  \"\"\"\n",
        "  # Dense features in Example proto.\n",
        "  feature_map = {\n",
        "      'image/encoded': tf.io.FixedLenFeature([], dtype=tf.string,\n",
        "                                             default_value=''),\n",
        "      'image/class/label': tf.io.FixedLenFeature([], dtype=tf.int64,\n",
        "                                                 default_value=-1),\n",
        "      'image/class/text': tf.io.FixedLenFeature([], dtype=tf.string,\n",
        "                                                default_value=''),\n",
        "  }\n",
        "  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n",
        "  # Sparse features in Example proto.\n",
        "  feature_map.update(\n",
        "      {k: sparse_float32 for k in ['image/object/bbox/xmin',\n",
        "                                   'image/object/bbox/ymin',\n",
        "                                   'image/object/bbox/xmax',\n",
        "                                   'image/object/bbox/ymax']})\n",
        "\n",
        "  features = tf.io.parse_single_example(serialized=example_serialized,\n",
        "                                        features=feature_map)\n",
        "  label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
        "\n",
        "  xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)\n",
        "  ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0)\n",
        "  xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0)\n",
        "  ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0)\n",
        "\n",
        "  # Note that we impose an ordering of (y, x) just to make life difficult.\n",
        "  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n",
        "\n",
        "  # Force the variable number of bounding boxes into the shape\n",
        "  # [1, num_boxes, coords].\n",
        "  bbox = tf.expand_dims(bbox, 0)\n",
        "  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n",
        "\n",
        "  return features['image/encoded'], label, bbox\n",
        "\n",
        "\n",
        "def parse_record(raw_record, is_training, dtype):\n",
        "  \"\"\"Parses a record containing a training example of an image.\n",
        "  The input record is parsed into a label and image, and the image is passed\n",
        "  through preprocessing steps (cropping, flipping, and so on).\n",
        "  Args:\n",
        "    raw_record: scalar Tensor tf.string containing a serialized\n",
        "      Example protocol buffer.\n",
        "    is_training: A boolean denoting whether the input is for training.\n",
        "    dtype: data type to use for images/features.\n",
        "  Returns:\n",
        "    Tuple with processed image tensor and one-hot-encoded label tensor.\n",
        "  \"\"\"\n",
        "  image_buffer, label, bbox = _parse_example_proto(raw_record)\n",
        "\n",
        "  image = imagenet_preprocessing.preprocess_image(\n",
        "      image_buffer=image_buffer,\n",
        "      bbox=bbox,\n",
        "      output_height=DEFAULT_IMAGE_SIZE,\n",
        "      output_width=DEFAULT_IMAGE_SIZE,\n",
        "      num_channels=NUM_CHANNELS,\n",
        "      is_training=is_training)\n",
        "  image = tf.cast(image, dtype)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "\n",
        "def input_fn(is_training, data_dir, batch_size, num_epochs=1,\n",
        "             dtype=tf.float32, datasets_num_private_threads=None,\n",
        "             num_parallel_batches=1, parse_record_fn=parse_record):\n",
        "  \"\"\"Input function which provides batches for train or eval.\n",
        "  Args:\n",
        "    is_training: A boolean denoting whether the input is for training.\n",
        "    data_dir: The directory containing the input data.\n",
        "    batch_size: The number of samples per batch.\n",
        "    num_epochs: The number of epochs to repeat the dataset.\n",
        "    dtype: Data type to use for images/features\n",
        "    datasets_num_private_threads: Number of private threads for tf.data.\n",
        "    num_parallel_batches: Number of parallel batches for tf.data.\n",
        "    parse_record_fn: Function to use for parsing the records.\n",
        "  Returns:\n",
        "    A dataset that can be used for iteration.\n",
        "  \"\"\"\n",
        "  filenames = get_filenames(is_training, data_dir)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "\n",
        "  if is_training:\n",
        "    # Shuffle the input files\n",
        "    dataset = dataset.shuffle(buffer_size=_NUM_TRAIN_FILES)\n",
        "\n",
        "  # Convert to individual records.\n",
        "  # cycle_length = 10 means 10 files will be read and deserialized in parallel.\n",
        "  # This number is low enough to not cause too much contention on small systems\n",
        "  # but high enough to provide the benefits of parallelization. You may want\n",
        "  # to increase this number if you have a large number of CPU cores.\n",
        "  dataset = dataset.apply(tf.data.experimental.parallel_interleave(\n",
        "      tf.data.TFRecordDataset, cycle_length=10))\n",
        "\n",
        "  return resnet_run_loop.process_record_dataset(\n",
        "      dataset=dataset,\n",
        "      is_training=is_training,\n",
        "      batch_size=batch_size,\n",
        "      shuffle_buffer=_SHUFFLE_BUFFER,\n",
        "      parse_record_fn=parse_record_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      dtype=dtype,\n",
        "      datasets_num_private_threads=datasets_num_private_threads,\n",
        "      num_parallel_batches=num_parallel_batches\n",
        "  )\n",
        "\n",
        "\n",
        "def get_synth_input_fn(dtype):\n",
        "  return resnet_run_loop.get_synth_input_fn(\n",
        "      DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE, NUM_CHANNELS, NUM_CLASSES,\n",
        "      dtype=dtype)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Running the model\n",
        "###############################################################################\n",
        "class ImagenetModel(resnet_model.Model):\n",
        "  \"\"\"Model class with appropriate defaults for Imagenet data.\"\"\"\n",
        "\n",
        "  def __init__(self, resnet_size, data_format=None, num_classes=NUM_CLASSES,\n",
        "               resnet_version=resnet_model.DEFAULT_VERSION,\n",
        "               dtype=resnet_model.DEFAULT_DTYPE):\n",
        "    \"\"\"These are the parameters that work for Imagenet data.\n",
        "    Args:\n",
        "      resnet_size: The number of convolutional layers needed in the model.\n",
        "      data_format: Either 'channels_first' or 'channels_last', specifying which\n",
        "        data format to use when setting up the model.\n",
        "      num_classes: The number of output classes needed from the model. This\n",
        "        enables users to extend the same model to their own datasets.\n",
        "      resnet_version: Integer representing which version of the ResNet network\n",
        "        to use. See README for details. Valid values: [1, 2]\n",
        "      dtype: The TensorFlow dtype to use for calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    # For bigger models, we want to use \"bottleneck\" layers\n",
        "    if resnet_size < 50:\n",
        "      bottleneck = False\n",
        "    else:\n",
        "      bottleneck = True\n",
        "\n",
        "    super(ImagenetModel, self).__init__(\n",
        "        resnet_size=resnet_size,\n",
        "        bottleneck=bottleneck,\n",
        "        num_classes=num_classes,\n",
        "        num_filters=64,\n",
        "        kernel_size=7,\n",
        "        conv_stride=2,\n",
        "        first_pool_size=3,\n",
        "        first_pool_stride=2,\n",
        "        block_sizes=_get_block_sizes(resnet_size),\n",
        "        block_strides=[1, 2, 2, 2],\n",
        "        resnet_version=resnet_version,\n",
        "        data_format=data_format,\n",
        "        dtype=dtype\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_block_sizes(resnet_size):\n",
        "  \"\"\"Retrieve the size of each block_layer in the ResNet model.\n",
        "  The number of block layers used for the Resnet model varies according\n",
        "  to the size of the model. This helper grabs the layer set we want, throwing\n",
        "  an error if a non-standard size has been selected.\n",
        "  Args:\n",
        "    resnet_size: The number of convolutional layers needed in the model.\n",
        "  Returns:\n",
        "    A list of block sizes to use in building the model.\n",
        "  Raises:\n",
        "    KeyError: if invalid resnet_size is received.\n",
        "  \"\"\"\n",
        "  choices = {\n",
        "      18: [2, 2, 2, 2],\n",
        "      34: [3, 4, 6, 3],\n",
        "      50: [3, 4, 6, 3],\n",
        "      101: [3, 4, 23, 3],\n",
        "      152: [3, 8, 36, 3],\n",
        "      200: [3, 24, 36, 3]\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    return choices[resnet_size]\n",
        "  except KeyError:\n",
        "    err = ('Could not find layers for selected Resnet size.\\n'\n",
        "           'Size received: {}; sizes allowed: {}.'.format(\n",
        "               resnet_size, choices.keys()))\n",
        "    raise ValueError(err)\n",
        "\n",
        "\n",
        "def imagenet_model_fn(features, labels, mode, params):\n",
        "  \"\"\"Our model_fn for ResNet to be used with our Estimator.\"\"\"\n",
        "\n",
        "  # Warmup and higher lr may not be valid for fine tuning with small batches\n",
        "  # and smaller numbers of training images.\n",
        "  if params['fine_tune']:\n",
        "    warmup = False\n",
        "    base_lr = .1\n",
        "  else:\n",
        "    warmup = True\n",
        "    base_lr = .128\n",
        "\n",
        "  learning_rate_fn = resnet_run_loop.learning_rate_with_decay(\n",
        "      batch_size=params['batch_size'], batch_denom=256,\n",
        "      num_images=NUM_IMAGES['train'], boundary_epochs=[30, 60, 80, 90],\n",
        "      decay_rates=[1, 0.1, 0.01, 0.001, 1e-4], warmup=warmup, base_lr=base_lr)\n",
        "\n",
        "  return resnet_run_loop.resnet_model_fn(\n",
        "      features=features,\n",
        "      labels=labels,\n",
        "      mode=mode,\n",
        "      model_class=ImagenetModel,\n",
        "      resnet_size=params['resnet_size'],\n",
        "      weight_decay=1e-4,\n",
        "      learning_rate_fn=learning_rate_fn,\n",
        "      momentum=0.9,\n",
        "      data_format=params['data_format'],\n",
        "      resnet_version=params['resnet_version'],\n",
        "      loss_scale=params['loss_scale'],\n",
        "      loss_filter_fn=None,\n",
        "      dtype=params['dtype'],\n",
        "      fine_tune=params['fine_tune']\n",
        "  )\n",
        "\n",
        "\n",
        "def define_imagenet_flags():\n",
        "  resnet_run_loop.define_resnet_flags(\n",
        "      resnet_size_choices=['18', '34', '50', '101', '152', '200'])\n",
        "  flags.adopt_module_key_flags(resnet_run_loop)\n",
        "  flags_core.set_defaults(train_epochs=90)\n",
        "\n",
        "\n",
        "def run_imagenet(flags_obj):\n",
        "  \"\"\"Run ResNet ImageNet training and eval loop.\n",
        "  Args:\n",
        "    flags_obj: An object containing parsed flag values.\n",
        "  \"\"\"\n",
        "  input_function = (flags_obj.use_synthetic_data and\n",
        "                    get_synth_input_fn(flags_core.get_tf_dtype(flags_obj)) or\n",
        "                    input_fn)\n",
        "\n",
        "  resnet_run_loop.resnet_main(\n",
        "      flags_obj, imagenet_model_fn, input_function, DATASET_NAME,\n",
        "      shape=[DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE, NUM_CHANNELS])\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  with logger.benchmark_context(flags.FLAGS):\n",
        "    run_imagenet(flags.FLAGS)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "  define_imagenet_flags()\n",
        "  absl_app.run(main)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bc2325cf2e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflags_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'official'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kg0B-xMVmlD6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}